================================================================================
GUIA: INSTALA√á√ÉO INCREMENTAL DOS 7 MODELOS IA (RTX 3090)
================================================================================
Sistema: JURIS_IA_CORE_V1
Estrat√©gia: Usar modelos existentes primeiro, escalar conforme necess√°rio
Data: 2025-12-17
================================================================================

FILOSOFIA
================================================================================

‚úÖ Use o que voc√™ J√Å TEM instalado
‚úÖ Instale novos modelos conforme NECESSIDADE
‚úÖ Escale para modelos maiores quando quiser MAIS QUALIDADE
‚úÖ Todos os 7 modelos s√£o OPCIONAIS - o sistema funciona com apenas 2


OS 7 MODELOS RECOMENDADOS
================================================================================

CATEGORIA 1: EMBEDDINGS (escolher 1 de 3)
------------------------------------------
Fun√ß√£o: Busca sem√¢ntica de quest√µes

1. ü•á nomic-embed-text (RECOMENDADO)
   Tamanho: 1.5GB
   Dimens√µes: 768
   Qualidade: Excelente
   Uso: Melhor para portugu√™s jur√≠dico
   Comando: ollama pull nomic-embed-text

2. ü•à mxbai-embed-large (alternativa)
   Tamanho: 670MB
   Dimens√µes: 1024
   Qualidade: Muito boa
   Uso: Mais r√°pido, menor
   Comando: ollama pull mxbai-embed-large

3. ü•â all-minilm (leve)
   Tamanho: 120MB
   Dimens√µes: 384
   Qualidade: Boa
   Uso: Testes r√°pidos
   Comando: ollama pull all-minilm


CATEGORIA 2: LLM PRINCIPAL (escolher 1 de 3)
---------------------------------------------
Fun√ß√£o: Explica√ß√µes pedag√≥gicas principais

4. ü•á llama3.1:8b (RECOMENDADO - melhor custo-benef√≠cio)
   Tamanho: 4.7GB
   Par√¢metros: 8 bilh√µes
   Qualidade: Muito boa
   Performance RTX 3090: 100-150 tokens/seg
   Comando: ollama pull llama3.1:8b

5. ü•â llama3.2:3b (mais leve, come√ßar aqui)
   Tamanho: 2GB
   Par√¢metros: 3 bilh√µes
   Qualidade: Boa
   Performance RTX 3090: 150-200 tokens/seg
   Comando: ollama pull llama3.2:3b

6. ü•à llama3:70b-q4_K_M (m√°xima qualidade)
   Tamanho: 40GB
   Par√¢metros: 70 bilh√µes (quantizado)
   Qualidade: Excelente (quase GPT-4)
   Performance RTX 3090: 30-50 tokens/seg
   Comando: ollama pull llama3:70b-q4_K_M


CATEGORIA 3: ESPECIALIZADOS (opcional)
---------------------------------------

7. üí° llama3.1:8b-instruct-q8_0 (instru√ß√µes refinadas)
   Tamanho: 8.5GB
   Uso: Vers√£o instru√≠da do 8B, melhor para explica√ß√µes complexas
   Performance RTX 3090: 80-120 tokens/seg
   Comando: ollama pull llama3.1:8b-instruct-q8_0


ESTRAT√âGIA DE INSTALA√á√ÉO INCREMENTAL
================================================================================

FASE 1: M√çNIMO VI√ÅVEL (3.5GB total)
------------------------------------
Instalar apenas 2 modelos para come√ßar:

‚úÖ OBRIGAT√ìRIO: 1 modelo de embedding
   Recomendado: all-minilm (120MB) OU nomic-embed-text (1.5GB)

‚úÖ OBRIGAT√ìRIO: 1 modelo LLM
   Recomendado: llama3.2:3b (2GB)

Comandos:
  # Op√ß√£o leve (3.5GB)
  ollama pull all-minilm
  ollama pull llama3.2:3b

  # Op√ß√£o balanceada (3.5GB)
  ollama pull nomic-embed-text
  ollama pull llama3.2:3b

Tempo download: ~5-10 minutos
Funcionalidade: 100% operacional


FASE 2: QUALIDADE MELHOR (6.2GB total)
---------------------------------------
Upgrade para modelos melhores:

Comandos:
  ollama pull nomic-embed-text    # Se ainda n√£o instalou
  ollama pull llama3.1:8b         # Upgrade do LLM

Tempo download: ~10-15 minutos
Melhoria: +50% qualidade nas explica√ß√µes


FASE 3: ESPECIALIZA√á√ÉO (14.7GB total)
--------------------------------------
Adicionar modelo instru√≠do para casos complexos:

Comando:
  ollama pull llama3.1:8b-instruct-q8_0

Tempo download: ~15 minutos
Melhoria: Explica√ß√µes mais precisas e estruturadas


FASE 4: M√ÅXIMA QUALIDADE (54.7GB total)
----------------------------------------
Para os melhores resultados (requer RTX 3090):

Comando:
  ollama pull llama3:70b-q4_K_M

Tempo download: ~60-90 minutos
Melhoria: Qualidade pr√≥xima a GPT-4


DETECTAR E USAR MODELOS J√Å INSTALADOS
================================================================================

O sistema detecta automaticamente os modelos que voc√™ j√° tem!

1. EXECUTAR SCRIPT DE DETEC√á√ÉO:

   cd D:\JURIS_IA_CORE_V1
   python scripts/detectar_modelos_ollama.py

   Isso vai:
   ‚úì Listar modelos j√° instalados
   ‚úì Recomendar quais usar
   ‚úì Sugerir pr√≥ximos a instalar
   ‚úì Atualizar .env automaticamente

2. SA√çDA EXEMPLO:

   üì¶ MODELOS J√Å INSTALADOS:
     ‚úì llama3.2:3b (2.0 GB)
     ‚úì nomic-embed-text:latest (1.5 GB)

   ‚úÖ USAR AGORA (j√° instalados):
     ‚Ä¢ Embedding: nomic-embed-text
       Melhor embedding para portugu√™s jur√≠dico

     ‚Ä¢ LLM Principal: llama3.2:3b
       R√°pido, bom para come√ßar
       Performance RTX 3090: 150-200 tokens/seg

   üî• INSTALAR PR√ìXIMOS (prioridade):
     1. LLM Principal: llama3.1:8b
        Melhor custo-benef√≠cio (RECOMENDADO)
        Tamanho: 4.7 GB
        Performance RTX 3090: 100-150 tokens/seg
        Comando: ollama pull llama3.1:8b

   ‚öôÔ∏è CONFIGURA√á√ÉO AUTO-DETECTADA:
     EMBEDDING_MODEL=nomic-embed-text
     LLM_MODEL=llama3.2:3b

   Deseja atualizar o arquivo .env? (s/n): s
   ‚úì Arquivo .env atualizado com sucesso!


COMANDOS R√ÅPIDOS
================================================================================

# Ver modelos j√° instalados
ollama list

# Detectar e configurar automaticamente
python scripts/detectar_modelos_ollama.py

# Instalar modelo espec√≠fico
ollama pull <nome-do-modelo>

# Remover modelo (se quiser trocar)
ollama rm <nome-do-modelo>

# Testar modelo
ollama run <nome-do-modelo> "Explique o princ√≠pio da isonomia"


TABELA DE DECIS√ÉO
================================================================================

Situa√ß√£o                              | Recomenda√ß√£o
--------------------------------------|--------------------------------------
"Quero come√ßar AGORA"                 | all-minilm + llama3.2:3b (3.5GB)
"J√° tenho llama3.2"                   | USE ELE! N√£o precisa baixar outro
"Quero boa qualidade"                 | nomic-embed + llama3.1:8b (6.2GB)
"Tenho RTX 3090 e quero o melhor"     | nomic-embed + llama3:70b (41.5GB)
"Preciso de explica√ß√µes complexas"    | Adicionar llama3.1:8b-instruct
"N√£o sei qual escolher"               | Execute detectar_modelos_ollama.py


WORKFLOW RECOMENDADO
================================================================================

DIA 1: Setup Inicial
---------------------
1. Verificar modelos j√° instalados:
   ollama list

2. Se j√° tem algum Llama:
   python scripts/detectar_modelos_ollama.py
   # Use o que j√° tem!

3. Se n√£o tem nenhum:
   ollama pull llama3.2:3b        # 2GB, r√°pido
   ollama pull nomic-embed-text   # 1.5GB

4. Configurar automaticamente:
   python scripts/detectar_modelos_ollama.py

5. Testar:
   make populate-embeddings-test  # 100 quest√µes


DIA 2-7: Uso Normal
-------------------
Use os modelos que voc√™ tem. Monitore:
- Qualidade das explica√ß√µes
- Velocidade
- Satisfa√ß√£o com resultados


SEMANA 2+: Escalar se Necess√°rio
---------------------------------
1. Se achar explica√ß√µes fracas:
   ollama pull llama3.1:8b  # Upgrade

2. Se precisar mais especializa√ß√£o:
   ollama pull llama3.1:8b-instruct-q8_0

3. Se quiser m√°xima qualidade:
   ollama pull llama3:70b-q4_K_M  # Requer RTX 3090


INTEGRA√á√ÉO COM DOCKER
================================================================================

Se usar Docker, a detec√ß√£o funciona igual:

# Detectar modelos no container Ollama
docker-compose exec backend python scripts/detectar_modelos_ollama.py

# Instalar novo modelo
docker-compose exec ollama ollama pull llama3.1:8b

# Reiniciar backend para usar novo modelo
docker-compose restart backend


MONITORAMENTO DE RECURSOS (RTX 3090)
================================================================================

Modelo          | VRAM Usada | Performance
----------------|------------|------------------
llama3.2:3b     | ~4GB       | 150-200 tokens/s
llama3.1:8b     | ~6GB       | 100-150 tokens/s
llama3:70b      | ~22GB      | 30-50 tokens/s
Todos + cache   | ~24GB      | Usa toda VRAM

Comando para monitorar:
  watch -n 1 nvidia-smi


PERGUNTAS FREQUENTES
================================================================================

P: Preciso instalar todos os 7 modelos?
R: N√ÉO! Apenas 2 s√£o obrigat√≥rios (1 embedding + 1 LLM). O resto √© opcional.

P: Qual o m√≠nimo para funcionar?
R: all-minilm (120MB) + llama3.2:3b (2GB) = 3.5GB total

P: Posso usar modelos que j√° tenho?
R: SIM! Execute detectar_modelos_ollama.py para usar automaticamente.

P: Como saber se preciso upgrade?
R: Se achar as explica√ß√µes muito simples ou imprecisas, fa√ßa upgrade.

P: Posso instalar v√°rios e escolher depois?
R: SIM! Instale v√°rios e configure qual usar no .env

P: RTX 3090 aguenta o modelo 70B?
R: SIM! Com 24GB VRAM voc√™ roda tranquilo.

P: Quanto espa√ßo preciso no disco?
R: M√≠nimo: 3.5GB | Recomendado: 10GB | Premium: 50GB

P: Posso remover modelos depois?
R: SIM! ollama rm <nome-do-modelo>


RESUMO EXECUTIVO
================================================================================

‚úÖ COMECE COM O QUE VOC√ä J√Å TEM
   python scripts/detectar_modelos_ollama.py

‚úÖ M√çNIMO VI√ÅVEL (3.5GB)
   all-minilm + llama3.2:3b

‚úÖ RECOMENDADO RTX 3090 (6.2GB)
   nomic-embed-text + llama3.1:8b

‚úÖ M√ÅXIMA QUALIDADE (41.5GB)
   nomic-embed-text + llama3:70b

‚úÖ N√ÉO PRECISA INSTALAR TODOS OS 7
   Apenas 2 obrigat√≥rios, resto √© opcional para escalar


PR√ìXIMOS PASSOS
================================================================================

1. [ ] Verificar modelos j√° instalados: ollama list
2. [ ] Executar detec√ß√£o: python scripts/detectar_modelos_ollama.py
3. [ ] Se necess√°rio, instalar m√≠nimo: ollama pull llama3.2:3b
4. [ ] Configurar: .env ser√° atualizado automaticamente
5. [ ] Testar: python scripts/popular_embeddings_ollama.py --limite 10
6. [ ] Usar por alguns dias
7. [ ] Escalar se necess√°rio: ollama pull llama3.1:8b


================================================================================
FIM DO GUIA
================================================================================
